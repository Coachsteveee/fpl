Attempting to run scraper...
[2024-11-23 16:18:35] === Starting FPL Scraper Process ===
[2024-11-23 16:18:35] Attempt 1 of 3
[2024-11-23 16:18:35] Starting scraper execution...
[2024-11-23 16:20:03] === Starting FPL Scraper Process ===
[2024-11-23 16:20:03] Attempt 1 of 3
[2024-11-23 16:20:03] Starting scraper execution...
[2024-11-23 16:24:08] Scraper execution timed out after 4 minutes
[2024-11-23 16:24:08] Scraper failed. Error: Error: Timeout
[2024-11-23 16:24:08] Waiting 60 seconds before retry...
[2024-11-23 16:24:13] Scraper execution timed out after 4 minutes
[2024-11-23 16:24:13] Scraper failed. Error: Error: Timeout
[2024-11-23 16:24:13] Waiting 60 seconds before retry...
[2024-11-23 16:26:28] === Starting FPL Scraper Process ===
[2024-11-23 16:26:28] Checking environment...
[2024-11-23 16:26:28] Testing Python installation...
[2024-11-23 16:26:28] Python version: Python 3.12.4
[2024-11-23 16:26:28] Changed to working directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:26:28] Running single scraper attempt for debugging...
[2024-11-23 16:26:28] Starting scraper execution...
[2024-11-23 16:26:28] Python script path: C:\Users\NUDDY\Documents\GitHub\fpl\test_scraper.py
[2024-11-23 16:26:28] Current directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:26:28] Creating background job...
[2024-11-23 16:26:28] Waiting for job completion (timeout: 10 minutes)...
[2024-11-23 16:26:38] Job completed, receiving output...
[2024-11-23 16:26:39] Job output: Starting Python execution... Test scraper starting... Processing step 1/5... Processing step 2/5... Processing step 3/5... Processing step 4/5... Processing step 5/5... Test scraper completed successfully
[2024-11-23 16:26:39] Job cleaned up
[2024-11-23 16:26:39] Scraper output: Starting Python execution... Test scraper starting... Processing step 1/5... Processing step 2/5... Processing step 3/5... Processing step 4/5... Processing step 5/5... Test scraper completed successfully
[2024-11-23 16:26:39] Script completed
[2024-11-23 16:28:17] === Starting FPL Scraper Process ===
[2024-11-23 16:28:17] Checking environment...
[2024-11-23 16:28:17] Testing Python installation...
[2024-11-23 16:28:17] Python version: Python 3.12.4
[2024-11-23 16:28:17] Changed to working directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:28:17] Running single scraper attempt for debugging...
[2024-11-23 16:28:18] Starting scraper execution...
[2024-11-23 16:28:18] Python script path: C:\Users\NUDDY\Documents\GitHub\fpl\test_scraper.py
[2024-11-23 16:28:18] Current directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:28:18] Creating background job...
[2024-11-23 16:28:18] Waiting for job completion (timeout: 10 minutes)...
[2024-11-23 16:29:25] === Starting FPL Scraper Process ===
[2024-11-23 16:29:25] Checking environment...
[2024-11-23 16:29:25] Testing Python installation...
[2024-11-23 16:29:25] Python version: Python 3.12.4
[2024-11-23 16:29:25] Changed to working directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:29:25] Running single scraper attempt for debugging...
[2024-11-23 16:29:25] Starting scraper execution...
[2024-11-23 16:29:25] Python script path: C:\Users\NUDDY\Documents\GitHub\fpl\test_scraper.py
[2024-11-23 16:29:25] Current directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:29:25] Creating background job...
[2024-11-23 16:29:25] Waiting for job completion (timeout: 10 minutes)...
[2024-11-23 16:29:39] Job completed, receiving output...
[2024-11-23 16:29:39] Job output: Starting Python execution... Test scraper starting... Test scraper completed successfully
[2024-11-23 16:29:39] Job cleaned up
[2024-11-23 16:29:39] Scraper output: Starting Python execution... Test scraper starting... Test scraper completed successfully
[2024-11-23 16:29:39] Script completed
[2024-11-23 16:29:58] === Starting FPL Scraper Process ===
[2024-11-23 16:29:58] Checking environment...
[2024-11-23 16:29:58] Testing Python installation...
[2024-11-23 16:29:58] Python version: Python 3.12.4
[2024-11-23 16:29:58] Changed to working directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:29:58] Running single scraper attempt for debugging...
[2024-11-23 16:29:58] Starting scraper execution...
[2024-11-23 16:29:58] Python script path: C:\Users\NUDDY\Documents\GitHub\fpl\test_scraper.py
[2024-11-23 16:29:58] Current directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:29:58] Creating background job...
[2024-11-23 16:29:59] Waiting for job completion (timeout: 10 minutes)...
[2024-11-23 16:30:03] === Starting FPL Scraper Process ===
[2024-11-23 16:30:03] Attempt 1 of 3
[2024-11-23 16:30:03] Starting scraper execution...
[2024-11-23 16:30:42] Scraper execution completed successfully
[2024-11-23 16:30:42] Scraper completed successfully
[2024-11-23 16:30:42] Running git pull...
[2024-11-23 16:30:43] Adding files to git...
[2024-11-23 16:30:43] Committing changes...
[2024-11-23 16:30:43] Git operation failed: Git commit failed: On branch main Your branch is up to date with 'origin/main'.  Changes not staged for commit:   (use "git add <file>..." to update what will be committed)   (use "git restore <file>..." to discard changes in working directory) 	modified:   runner.ps1 	modified:   scraper_log.txt  Untracked files:   (use "git add <file>..." to include in what will be committed) 	debugger.ps1 	test_scraper.py  no changes added to commit (use "git add" and/or "git commit -a")
[2024-11-23 16:30:43] =====================================================================
Run Time: 11/23/2024 16:30:03
Duration: 00:00:39.9035215
End Time: 11/23/2024 16:30:43
Status: Partial Success (Git Failed)
Attempts: 1
=====================================================================
[2024-11-23 16:31:14] === Starting FPL Scraper Process ===
[2024-11-23 16:31:14] Checking environment...
[2024-11-23 16:31:14] Testing Python installation...
[2024-11-23 16:31:14] Python version: Python 3.12.4
[2024-11-23 16:31:14] Changed to working directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:31:14] Running single scraper attempt for debugging...
[2024-11-23 16:31:14] Starting scraper execution...
[2024-11-23 16:31:14] Python script path: C:\Users\NUDDY\Documents\GitHub\fpl\test_scraper.py
[2024-11-23 16:31:14] Current directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:31:14] Creating background job...
[2024-11-23 16:31:14] Waiting for job completion (timeout: 10 minutes)...
[2024-11-23 16:31:37] Job completed, receiving output...
[2024-11-23 16:31:37] Job output: Starting Python execution... Test scraper starting... Test scraper completed successfully
[2024-11-23 16:31:37] Job cleaned up
[2024-11-23 16:31:37] Scraper output: Starting Python execution... Test scraper starting... Test scraper completed successfully
[2024-11-23 16:31:37] Script completed
[2024-11-23 16:32:06] === Starting FPL Scraper Process ===
[2024-11-23 16:32:06] Checking environment...
[2024-11-23 16:32:06] Testing Python installation...
[2024-11-23 16:32:06] Python version: Python 3.12.4
[2024-11-23 16:32:06] Changed to working directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:32:06] Running single scraper attempt for debugging...
[2024-11-23 16:32:06] Starting scraper execution...
[2024-11-23 16:32:06] Python script path: C:\Users\NUDDY\Documents\GitHub\fpl\scraper.py
[2024-11-23 16:32:06] Current directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:32:06] Creating background job...
[2024-11-23 16:32:07] Waiting for job completion (timeout: 4 minutes)...
[2024-11-23 16:32:37] === Starting FPL Scraper Process ===
[2024-11-23 16:32:37] Checking environment...
[2024-11-23 16:32:37] Testing Python installation...
[2024-11-23 16:32:37] Python version: Python 3.12.4
[2024-11-23 16:32:37] Changed to working directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:32:37] Running single scraper attempt for debugging...
[2024-11-23 16:32:37] Starting scraper execution...
[2024-11-23 16:32:37] Python script path: C:\Users\NUDDY\Documents\GitHub\fpl\scraper.py
[2024-11-23 16:32:37] Current directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:32:37] Creating background job...
[2024-11-23 16:32:38] Waiting for job completion (timeout: 4 minutes)...
[2024-11-23 16:40:03] === Starting FPL Scraper Process ===
[2024-11-23 16:40:03] Checking environment...
[2024-11-23 16:40:03] Testing Python installation...
[2024-11-23 16:40:03] Python version: Python 3.12.4
[2024-11-23 16:40:03] Changed to working directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:40:03] Running single scraper attempt for debugging...
[2024-11-23 16:40:03] Starting scraper execution...
[2024-11-23 16:40:03] Python script path: C:\Users\NUDDY\Documents\GitHub\fpl\scraper.py
[2024-11-23 16:40:03] Current directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:40:03] Creating background job...
[2024-11-23 16:40:04] Waiting for job completion (timeout: 4 minutes)...
[2024-11-23 16:41:21] === Starting FPL Scraper Process ===
[2024-11-23 16:41:21] Changed to working directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:41:21] Cleaning up processes...
[2024-11-23 16:41:21] Killed process: firefox (ID: 1028)
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Cannot process request because the process (1448) has exited."
[2024-11-23 16:41:21] Killed process: firefox (ID: 2328)
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Access is denied"
[2024-11-23 16:41:21] Killed process: firefox (ID: 3360)
[2024-11-23 16:41:21] Killed process: firefox (ID: 10812)
[2024-11-23 16:41:21] Killed process: firefox (ID: 11408)
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Access is denied"
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Cannot process request because the process (14616) has exited."
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Cannot process request because the process (15436) has exited."
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Access is denied"
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Access is denied"
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Access is denied"
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Access is denied"
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Cannot process request because the process (18536) has exited."
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Cannot process request because the process (20088) has exited."
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Cannot process request because the process (20124) has exited."
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Cannot process request because the process (20568) has exited."
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Access is denied"
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Cannot process request because the process (21160) has exited."
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Access is denied"
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Cannot process request because the process (21556) has exited."
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Access is denied"
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Access is denied"
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Cannot process request because the process (22104) has exited."
[2024-11-23 16:41:21] Error killing process : Exception calling "Kill" with "0" argument(s): "Access is denied"
[2024-11-23 16:41:22] Killed process: geckodriver (ID: 13892)
[2024-11-23 16:41:22] Cleanup completed
[2024-11-23 16:41:22] Starting scraper execution...
[2024-11-23 16:41:25] Job completed, receiving output...
[2024-11-23 16:41:25] Job output: Starting Python execution... Traceback (most recent call last):   File "C:\Users\NUDDY\Documents\GitHub\fpl\scraper.py", line 120, in <module>     run_scraper()   File "C:\Users\NUDDY\Documents\GitHub\fpl\scraper.py", line 114, in run_scraper     print(df)   File "C:\Users\NUDDY\anaconda3\Lib\encodings\cp1252.py", line 19, in encode     return codecs.charmap_encode(input,self.errors,encoding_table)[0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ UnicodeEncodeError: 'charmap' codec can't encode character '\ufffd' in position 406: character maps to <undefined>
[2024-11-23 16:41:25] Job cleaned up
[2024-11-23 16:41:25] Scraper output: Starting Python execution... Traceback (most recent call last):   File "C:\Users\NUDDY\Documents\GitHub\fpl\scraper.py", line 120, in <module>     run_scraper()   File "C:\Users\NUDDY\Documents\GitHub\fpl\scraper.py", line 114, in run_scraper     print(df)   File "C:\Users\NUDDY\anaconda3\Lib\encodings\cp1252.py", line 19, in encode     return codecs.charmap_encode(input,self.errors,encoding_table)[0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ UnicodeEncodeError: 'charmap' codec can't encode character '\ufffd' in position 406: character maps to <undefined>
[2024-11-23 16:41:25] Script completed
[2024-11-23 16:43:53] === Starting FPL Scraper Process ===
[2024-11-23 16:43:53] Changed to working directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:43:53] Cleaning up processes...
[2024-11-23 16:43:53] Cleanup completed
[2024-11-23 16:43:53] Starting scraper execution...
[2024-11-23 16:48:41] Cleaning up processes...
[2024-11-23 16:48:41] Cleanup completed
[2024-11-23 16:48:41] Cleaning up processes...
[2024-11-23 16:48:41] Cleanup completed
[2024-11-23 16:48:41] Script execution completed
[2024-11-23 16:48:48] Starting scraper... 
[2024-11-23 16:50:04] === Starting FPL Scraper Process ===
[2024-11-23 16:50:04] Changed to working directory: C:\Users\NUDDY\Documents\GitHub\fpl
[2024-11-23 16:50:04] Cleaning up processes...
[2024-11-23 16:50:04] Killed process: geckodriver (ID: 23096)
[2024-11-23 16:50:04] Cleanup completed
[2024-11-23 16:50:04] Starting scraper execution...
[2024-11-23 16:48:48] Scraper completed successfully 
[2024-11-23 16:48:48] Process completed 
[2024-11-23 16:50:38] Scraper execution completed.
[2024-11-23 16:50:40] Cleaning up processes...
[2024-11-23 16:50:40] Cleanup completed
=====================================================================
Run Time: 11/23/2024 16:50:04
Duration: 00:00:36.7816886
End Time: 11/23/2024 16:50:40
Status: Fail
Output:
Traceback (most recent call last):   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\urllib3\connection.py", line 196, in _new_conn     sock = connection.create_connection(            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\urllib3\util\connection.py", line 85, in create_connection     raise err   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\urllib3\util\connection.py", line 73, in create_connection     sock.connect(sa) ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it System.Management.Automation.RemoteException The above exception was the direct cause of the following exception: System.Management.Automation.RemoteException Traceback (most recent call last):   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\urllib3\connectionpool.py", line 789, in urlopen     response = self._make_request(                ^^^^^^^^^^^^^^^^^^^   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\urllib3\connectionpool.py", line 495, in _make_request     conn.request(   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\urllib3\connection.py", line 398, in request     self.endheaders()   File "C:\Users\NUDDY\anaconda3\Lib\http\client.py", line 1331, in endheaders     self._send_output(message_body, encode_chunked=encode_chunked)   File "C:\Users\NUDDY\anaconda3\Lib\http\client.py", line 1091, in _send_output     self.send(msg)   File "C:\Users\NUDDY\anaconda3\Lib\http\client.py", line 1035, in send     self.connect()   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\urllib3\connection.py", line 236, in connect     self.sock = self._new_conn()                 ^^^^^^^^^^^^^^^^   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\urllib3\connection.py", line 211, in _new_conn     raise NewConnectionError( urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001C5A64C7320>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it System.Management.Automation.RemoteException The above exception was the direct cause of the following exception: System.Management.Automation.RemoteException Traceback (most recent call last):   File "C:\Users\NUDDY\Documents\GitHub\fpl\scraper.py", line 120, in <module>     run_scraper()   File "C:\Users\NUDDY\Documents\GitHub\fpl\scraper.py", line 111, in run_scraper     player_pos, team_name, player_name, points_for_gw = scrape()                                                         ^^^^^^^^   File "C:\Users\NUDDY\Documents\GitHub\fpl\scraper.py", line 72, in scrape     points_for_gw.append(int(driver.find_element(By.XPATH, "//div[contains(@class, 'EntryEvent__PrimaryValue-sc-l17rqm-4 jsdnqB')]").text))                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\selenium\webdriver\remote\webdriver.py", line 831, in find_element     return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\selenium\webdriver\remote\webdriver.py", line 438, in execute     response = self.command_executor.execute(driver_command, params)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 290, in execute     return self._request(command_info[0], url, body=data)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 311, in _request     response = self._conn.request(method, url, body=body, headers=headers)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\urllib3\_request_methods.py", line 144, in request     return self.request_encode_body(            ^^^^^^^^^^^^^^^^^^^^^^^^^   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\urllib3\_request_methods.py", line 279, in request_encode_body     return self.urlopen(method, url, **extra_kw)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\urllib3\poolmanager.py", line 443, in urlopen     response = conn.urlopen(method, u.request_uri, **kw)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen     return self.urlopen(            ^^^^^^^^^^^^^   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen     return self.urlopen(            ^^^^^^^^^^^^^   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen     return self.urlopen(            ^^^^^^^^^^^^^   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\urllib3\connectionpool.py", line 843, in urlopen     retries = retries.increment(               ^^^^^^^^^^^^^^^^^^   File "C:\Users\NUDDY\anaconda3\Lib\site-packages\urllib3\util\retry.py", line 519, in increment     raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=49723): Max retries exceeded with url: /session/d7fcea47-f9cd-4124-bf08-9551b67888c2/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001C5A64C7320>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
=====================================================================
[2024-11-23 16:50:40] Cleaning up processes...
[2024-11-23 16:50:40] Cleanup completed
[2024-11-23 16:50:40] Script execution completed
